# -*- coding: utf-8 -*-
"""sarcasm_e2e.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pWdMA_wmUCVyx3DTuLBHBtu6zYx5S6lq
"""

#necessary imports
import tensorflow as tf
import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string 
import re
import nltk
import os
import json

nltk.download('stopwords')
nltk.download('punkt')

#reading dataframe
df1 = pd.read_json("/data/Sarcasm_Headlines_Dataset.json",lines=True)
df2 = pd.read_json("/data/Sarcasm_Headlines_Dataset_v2.json",lines=True)

df1.count()

df2.count()

df = pd.concat([df1,df2])
df.count()

#removing duplicates
df = df.drop_duplicates().reset_index(drop=True)

df.count()

df.head()

def clean_text(text):

  text = text.lower() #converted to lowercase

  pattern = re.compile('https?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+') #removing link
  text = pattern.sub('', text) #replacing link with whitespace
  emoji = re.compile("["
                           u"\U0001F600-\U0001FFFF"  # removing emoji, symbols, flags
                           u"\U0001F300-\U0001F5FF"  
                           u"\U0001F680-\U0001F6FF"  
                           u"\U0001F1E0-\U0001F1FF"  
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
  text = emoji.sub('',text)
  text = re.sub(r"[,.\"\'!@#$%^&*(){}?/;`~:<>+=-]", "", text) #additional special characters removed
  return text

def token_word(dframe):

  head_line = list() #new list
  lines = dframe['headline'].values.tolist() #df values to list

  for line in lines:
    line = clean_text(line) #passing each insatnce of corpus 
    tokenize = word_tokenize(line) #NLTK tokenize function
    pure_words = [word for word in tokenize if word.isalpha()] #keeping only alphabets
    stop_words = set(stopwords.words("english")) #loading 'English' stopwords
    filtered_words = [ word for word in pure_words if not word in stop_words] #removing all stopwords
    head_line.append(filtered_words) #added to the list

  return head_line

head_lines = token_word(df)
head_lines[:5]

tokenizer_obj = tf.keras.preprocessing.text.Tokenizer() #tokenizer object
tokenizer_obj.fit_on_texts(head_lines) #Tokenizer fit method
word_index = tokenizer_obj.word_index #to count words
print(f'Unique words ', len(word_index))

sequences = tokenizer_obj.texts_to_sequences(head_lines)
lines_pad = tf.keras.preprocessing.sequence.pad_sequences(sequences=sequences, maxlen=25,padding='post') #to make sure each input has same length

sentiment = df['is_sarcastic'].values #extract output values

#shuffled to make sure no bias
dimen = np.arange(lines_pad.shape[0]) 
np.random.shuffle(dimen)
lines_pad = lines_pad[dimen]
sentiment = sentiment[dimen]

test_sample = int(0.2 * lines_pad.shape[0])

#train-test split
x_train = lines_pad[:-test_sample]
y_train = sentiment[:-test_sample]
x_test = lines_pad[-test_sample:]
y_test = sentiment[-test_sample:]

print(f' x_train = {x_train.shape} y_train = {y_train.shape} x_test = {x_test.shape} y_test = {y_test.shape} ')

# ls

os.chdir("drive/My Drive/sarcasm")

# ls

#creating dict. with keys:word and values:vector
embeddings_index = {}
embedding_dim = 100
f = open('glove.twitter.27B.100d.txt', encoding = "utf-8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

#creating embedding matrix to use in embedding layer
embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
c = 0
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        c+=1
        embedding_matrix[i] = embedding_vector
print(c)

type(word_index)

#Model architecture with pre-trained Glove embedding
model = tf.keras.models.Sequential([
                                    tf.keras.layers.Embedding(input_dim=len(word_index)+1,output_dim=embedding_dim,input_length=25,
                                                              embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False),
                                    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64,dropout=0.4,recurrent_dropout=0.25)),
                                    tf.keras.layers.Dense(units=1,activation='sigmoid')
])

optimizer= tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics='acc')

model.summary()

history= model.fit(x=x_train,y=y_train,batch_size=32,epochs=6,validation_data=(x_test,y_test),verbose=2)

# ls

# pwd

model.save('my_model.h5')

